{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fa37dd",
   "metadata": {},
   "source": [
    "# The Adam optimizer\n",
    "\n",
    "The primary usefulness of the adam optimizer is its improved capabilites over other optimizers in almost all cases. To demonstrate this improvement we are going to implement several different optimizers on a basic toy problem.\n",
    "\n",
    "### But first, some set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58d8bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "# function for measuring time taken curtises of ChatGPT (!MAY NOT USE, WE SHALL SEE)\n",
    "@contextmanager\n",
    "def time_block(label=\"Elapsed time\"):\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{label}: {end - start:.6f} seconds\")\n",
    "\n",
    "# function for measuring accuracy\n",
    "num_classes = 10\n",
    "acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "def get_acc(X_test, y_test):\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = classifier(torch.from_numpy(X_test).float())  # [899, 10] \n",
    "        y_true = torch.tensor(y_test)                           # [899]\n",
    "    \n",
    "        # Convert logits â†’ predicted class indices\n",
    "        y_pred_labels = torch.argmax(y_pred, dim=1)             # [899]\n",
    "    \n",
    "    accuracy = acc(y_pred_labels, y_true).item()\n",
    "    \n",
    "    print(\n",
    "        f\"Final accuracy: {accuracy:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b590177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My slow code: 0.747598 seconds\n"
     ]
    }
   ],
   "source": [
    "with time_block(\"My slow code\"):\n",
    "    total = sum(i * i for i in range(10_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49075d",
   "metadata": {},
   "source": [
    "### Toy digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "944815f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1)) #flattens the data\n",
    "X_train, X_test, y_train, y_test = train_test_split( #splits into training and testing sets\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84d9665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec2935",
   "metadata": {},
   "source": [
    "### Basic Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bc0cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.classification import Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class classifier_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Linear(input_size,output_size)) # single layer model\n",
    "        \n",
    "    def forward(self,X):\n",
    "        z = self.layer1(X)         # Apply the first layer (and only)\n",
    "        return(z)            # Return the result (and the latent space variable)\n",
    "            \n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "\n",
    "L = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b2b5207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008544730517154124"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training\n",
    "def train_model(L,optimizer,classifier, X_train, y_train, display=False):\n",
    "    n_epochs = 15\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        for X,y in zip(X_train,y_train): #y are labels\n",
    "            labels = torch.tensor(y)\n",
    "            inputs = torch.from_numpy(X).to(torch.float32)\n",
    "        \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = classifier(inputs) #gets the outputs of the model\n",
    "            train_loss = L(outputs,labels) #gets loss\n",
    "            #print(train_loss)\n",
    "            train_loss.backward() # compute accumulated gradients\n",
    "            optimizer.step()# perform parameter update based on current gradients\n",
    "            loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "        loss = loss / len(X_train) # compute the epoch training loss\n",
    "        if display:\n",
    "            print(f\"epoch : {epoch + 1}/{n_epochs}, loss = {loss}\")\n",
    "    return loss #this is the final loss at the end of all the training process\n",
    "train_model(L,optimizer,classifier, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d18794-0d0d-4c5b-95f4-a315dbf68395",
   "metadata": {},
   "source": [
    "### Gradient Scaling\n",
    "Unlike some basic optimizers, the Adam Optimizer is invariant to gradient scaling, so the default learning rate works well for most datasets. Run the optimizer on the dataset scaled datasets and compare the final accuracy. Is there any scalar that causes a noticable decrease in performance? Then, try the same (or different) scalars on Stochastic Gradient Descent. When does accuracy start to drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "047d81f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9121\n",
      "This is the final loss of the model: 0.0106 \n",
      "Adam optimizer: 2.321951 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "\"\"\"\n",
    "Implement the Adam Optimizer here\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train, y_train)\n",
    "    get_acc(X_test, y_test)\n",
    "    print(\n",
    "        f\"This is the final loss of the model: {final_loss:.4f} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c58cc4be-3071-4fea-ab15-8dcf6014d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9043\n",
      "Adam optimizer: 2.299483 seconds\n",
      "Final accuracy: 0.8810\n",
      "Adam optimizer: 2.303372 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scale the training/testing data. Compare the accuracies.\n",
    "\"\"\"\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "X_train_scaled = X_train/50\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train_scaled, y_train)\n",
    "    get_acc(X_test/50, y_test)\n",
    "\n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "X_train_scaled = X_train*50\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train_scaled, y_train)\n",
    "    get_acc(X_test*50, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ded60-c060-4426-8910-e3c5f8c9480c",
   "metadata": {},
   "source": [
    "#### Compare this with regular Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "71d3726a-7c13-4375-82b4-fa5e142195ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9166\n",
      "SGD optimizer: 1.641946 seconds\n",
      "Final accuracy: 0.1079\n",
      "SGD optimizer: 1.532072 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "\"\"\"\n",
    "Now, implement and run Stochastic Gradient Descent with at least twice with different scalars. Compare the change in accuracy to the Adam Optimizer.\n",
    "\"\"\"\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=1e-3) \n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train*(1/5), y_train)\n",
    "    get_acc(X_test*(1/5), y_test)\n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train*(5), y_train)\n",
    "    get_acc(X_test*(5), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbf55b-50c5-4f18-b0b1-b244e5e275df",
   "metadata": {},
   "source": [
    "If scaling does not affect the data, what about normalization? Compare the accuracy of the model before and after normalization. As a bonus, implement both with cross validation for more stable results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "864b700d-6b8c-47a0-ab9b-9bd61af37961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9299\n",
      "Adam optimizer: 2.296721 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Normalize the training/testing data and run the Adam Optimizer\n",
    "\"\"\"\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mean = np.mean(X_train, axis=0, keepdims=True) \n",
    "std  = np.std(X_train, axis=0, keepdims=True)   \n",
    "\n",
    "eps = 1e-8\n",
    "X_train_scaled = (X_train - mean) / (std + eps)\n",
    "X_test_scaled  = (X_test  - mean) / (std + eps)\n",
    "\n",
    "\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train_scaled, y_train)\n",
    "    get_acc(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d77318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.8977\n",
      "RMSprop optimizer: 2.095619 seconds\n"
     ]
    }
   ],
   "source": [
    "#torch.optim.LBFGS\n",
    "#torch.optim.RMSprop\n",
    "#import torchmin #this is how you get the newtons method full \n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.RMSprop(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"RMSprop optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train, y_train)\n",
    "    get_acc(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feaef0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 0.024601183763795054\n",
      "SGD optimizer: 0.868933 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "#train_model(L,optimizer,classifier)\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cedefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.4727\n",
      "This is the final loss of the model: 1.31173566483347\n",
      "Adagrad optimizer: 2.018979 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adagrad(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "#train_model(L,optimizer,classifier)\n",
    "with time_block(\"Adagrad optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train, y_train)\n",
    "    get_acc(X_test, y_test)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128d62c",
   "metadata": {},
   "source": [
    "# The Speed of the hessian matrix\n",
    "\n",
    "One would think that if newtons method is so exact that we would want to use it more often. Unfortunualy the hessian is a slow matrix to calculate as you will see here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccbe84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative df/dx: 12.0\n",
      "First derivative df/dy: 4.0\n",
      "Second derivative d^2f/dx^2: 6.0\n",
      "Mixed derivative d^2f/dydx: 4.0\n",
      "Mixed derivative d^2f/dxdy: 4.0\n"
     ]
    }
   ],
   "source": [
    "#inputs\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "def f(a, b):\n",
    "    return a**2 * b\n",
    "\n",
    "output = f(x, y)\n",
    "df_dx = torch.autograd.grad(outputs=output, inputs=x, create_graph=True)[0]\n",
    "print(f\"First derivative df/dx: {df_dx.item()}\")\n",
    "\n",
    "df_dy = torch.autograd.grad(outputs=output, inputs=y, create_graph=True)[0]\n",
    "print(f\"First derivative df/dy: {df_dy.item()}\")\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Second derivative d^2f/dy^2: {d2f_dy2.item()}\")\n",
    "\n",
    "d2f_dx2 = torch.autograd.grad(outputs=df_dx, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Second derivative d^2f/dx^2: {d2f_dx2.item()}\")\n",
    "\n",
    "d2f_dydx = torch.autograd.grad(outputs=df_dx, inputs=y,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dydx: {d2f_dydx.item()}\")\n",
    "\n",
    "d2f_dxdy = torch.autograd.grad(outputs=df_dy, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dxdy: {d2f_dxdy.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Mixed derivative d2f_dy2: {d2f_dy2.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59aaf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b3655b",
   "metadata": {},
   "source": [
    "Some sore of activtiyy where they calucate the full hessian, the code isnt finalized or even working right now but thats the concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e40d62d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_functional' from 'torch.func' (/Users/noahwanless/Desktop/Spring2026/CSCI457/.venv/lib/python3.11/site-packages/torch/func/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m targets = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 3. Extract parameters as a flat dictionary or tuple (functorch format)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Note: make_functional is a utility to easily get functional model and params\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_functional\n\u001b[32m     20\u001b[39m fnet, params = make_functional(model)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 4. Compute the Hessian\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# We need to wrap the loss computation in a function that takes params as input\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'make_functional' from 'torch.func' (/Users/noahwanless/Desktop/Spring2026/CSCI457/.venv/lib/python3.11/site-packages/torch/func/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import hessian, jacfwd, jacrev\n",
    "\n",
    "# 1. Define a function that returns a single scalar loss\n",
    "def compute_loss(model_params, input_data, targets, model, loss_fn):\n",
    "    # Use functional_call to call the model with specific parameters\n",
    "    outputs = torch.func.functional_call(model, model_params, input_data)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    return loss\n",
    "\n",
    "# 2. Instantiate your model and loss function\n",
    "model = torch.nn.Linear(10, 1) # Example model\n",
    "loss_fn = torch.nn.MSELoss()    # Example loss function\n",
    "input_data = torch.randn(1, 10)\n",
    "targets = torch.randn(1, 1)\n",
    "\n",
    "# 3. Extract parameters as a flat dictionary or tuple (functorch format)\n",
    "# Note: make_functional is a utility to easily get functional model and params\n",
    "from torch.func import make_functional\n",
    "fnet, params = make_functional(model)\n",
    "\n",
    "# 4. Compute the Hessian\n",
    "# We need to wrap the loss computation in a function that takes params as input\n",
    "def get_hessian_func(params):\n",
    "    return compute_loss(params, input_data, targets, model, loss_fn)\n",
    "\n",
    "# Calculate the full Hessian matrix\n",
    "# argnums=0 specifies we want the Hessian with respect to the first argument (params)\n",
    "full_hessian = hessian(get_hessian_func, argnums=0)(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36eb9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1208,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.9627,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.4295,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.1072,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.8535]])\n"
     ]
    }
   ],
   "source": [
    "def test_func(x):\n",
    "    return x.sin().sum()\n",
    "x = torch.randn(5) #takes in 5 inputs, so 5x5 hessian\n",
    "\n",
    "full_hessian = hessian(test_func, argnums=0)(x)\n",
    "print(full_hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eae01d",
   "metadata": {},
   "source": [
    "The Hessian encodes local curvature of the objective function, it allows the gradient to be scaled so that 'dierctions' that have little movement are downscaled, while directions with more curvuture are scaled up so more progress is made"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
