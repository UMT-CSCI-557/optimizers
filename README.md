# Adam (Adaptive Moment Estimator) Optimizer

## What is Adam?

Adam (Adaptive Moment Estimation) is an algorithm for first-order gradient based optimization of stochastic objective functions based on adaptive estimates of lower-order moments.

Some of Adamâ€™s advantages are:

1. The magnitudes of parameter updates are invariant to rescaling of the gradient 
2. Its stepsizes are approximately bounded by the stepsize hyperparameter 
3. It does not require a stationary objective (i.e., it will still converge if f(Ï‘) changes) 
4. It works with sparse gradients
5. It naturally performs a form of step size annealing

(Insert pseudocode picture here)

## Moments

First moment ($$ğ‘š_ğ‘¡$$): the mean of the gradient

$$ğ‘š_ğ‘¡= {\beta _1} â‹… m_{ğ‘¡âˆ’1}+(1 âˆ’ {\beta _1})â‹… ğ‘”_ğ‘¡$$

Second moment ($$ğ‘£_ğ‘¡$$): the raw, uncentered variance of the gradient

$$ğ‘£_ğ‘¡= ğ›½_2 â‹… ğ‘£_{ğ‘¡âˆ’1} + (1 âˆ’ {\beta_2})â‹… ğ‘”_ğ‘¡^2$$

Î²s (decay rates) give more weight to recent gradients

However, these moments are **BIASED** early on because they are initialized at 0

Hence, we have to correct the bias

$$\hat{ğ‘š_ğ‘¡}= ğ‘š_ğ‘¡âˆ•(1 âˆ’ {\beta_1^ğ‘¡})$$ 
$$\hat{ğ‘£_ğ‘¡}= ğ‘£_ğ‘¡âˆ•(1 âˆ’ {\beta_2^ğ‘¡})$$ 

But where does this bias correction come from?

$$ğ‘£_ğ‘¡=(1 âˆ’ ğ›½_2 ) âˆ‘_(ğ‘–=1)^ğ‘¡â–’ğ›½_2^(ğ‘¡ âˆ’1)   â‹… ğ‘”_ğ‘–^2$$

$$ğ”¼[ğ‘£_ğ‘¡] = ğ”¼[(1 âˆ’ ğ›½_2 ) âˆ‘_(ğ‘–=1)^ğ‘¡â–’ğ›½_2^(ğ‘¡ âˆ’1) â‹… ğ‘”_ğ‘–^2 ]$$

$$ğ”¼[ğ‘£_ğ‘¡ ]=ğ”¼[ğ‘”_ğ‘–^2 ]  â‹…(1 âˆ’ ğ›½_2 ) âˆ‘_(ğ‘–=1)^ğ‘¡â–’ğ›½_2^(ğ‘¡ âˆ’1) + Î¶$$

$$âˆ‘_(ğ‘–=1)^ğ‘¡â–’ğ›½_2^(ğ‘¡ âˆ’1) =  ((1 âˆ’ ğ›½_2^ğ‘¡))/((1 âˆ’ ğ›½_2))$$

What the paper fails to make explicit is that the summation here is actually a **geometric series** (i.e., when summing those values, subsequent terms are related to each other by a specific ratio, in this case, $\frac{1}{\beta}$. Therefore, we can sub in the ratio $\frac{1 âˆ’ {\beta_2^ğ‘¡}}{1 âˆ’ {\beta_2}}$ for the summation.

$$ğ”¼[ğ‘£_ğ‘¡ ]=ğ”¼[ğ‘”_ğ‘–^2 ]  â‹…(1 âˆ’ ğ›½_2 )  ((1 âˆ’ ğ›½_2^ğ‘¡))/((1 âˆ’ ğ›½_2))+ Î¶$$

And then we can arrive at the final equation shown in the paper by cancelling out the $1 - {\beta_2}$:

$$ğ”¼[ğ‘£_ğ‘¡ ]=ğ”¼[ğ‘”_ğ‘–^2 ]  â‹…(1 âˆ’ğ›½_2^ğ‘¡ )+ Î¶$$

So, because of this extra $1 - {\beta_2^t}$ that appears in this equation, that's why we divide $m_t$ and $v_t$ by $1 - {\beta_2^t}$.

## Step Size

Â The change in our step size at time t is:

$${\delta_ğ‘¡} = \alpha â‹… \frac{\hat{ğ‘š_ğ‘¡}}{\sqrt{\hat{ğ‘£_ğ‘¡}}}$$

, where ğ›¼ is a â€œmaximumâ€ step size parameter
If you want to take N steps to the optimum that is D distance away, then ğ›¼ â‰…  $\frac{ğ·}{ğ‘}$

The effective step sizes are approximately bound to the step size hyperparameter such that :

|Î”_ğ‘¡ |â‰¤{â–ˆ(ğ›¼ Â·(1 âˆ’ ğ›½_1 )âˆ•âˆš(1 âˆ’ ğ›½_2 ),  (1 âˆ’ ğ›½_1 )>âˆš(1 âˆ’ ğ›½_2 )@&ğ›¼,                                             (1 âˆ’ ğ›½_1 )â‰¤âˆš(1 âˆ’ ğ›½_2 ))â”¤
 Â  Â  Â  Â Â  Â  Â  Â  Â  Â  Â 
					      
Therefore, the step size will not grow too large except in the case of severe sparsity (when a gradient has been zero at all timesteps except at the current timestep)

ğ‘šÂ Ì‚_ğ‘¡âˆ•âˆš(ğ‘£Â Ì‚_ğ‘¡ ) is considered to be a signal-to-noise ratio (SNR)
When SNR is small, the step size decreases
SNR typically decreases when approaching an optimum, where we want smaller effective steps

Since the final step equation divides the estimated mean by the estimated variance (1st moment / 2nd moment), any gradient scaling cancels outâ€‹

ã€–(ğ‘ â‹… ğ‘šÂ Ì‚_ğ‘¡)ã€—âˆ•ã€–âˆš((ğ‘^2  â‹… ğ‘£Â Ì‚_ğ‘¡))= ã€— ğ‘šÂ Ì‚_ğ‘¡âˆ•âˆš(ğ‘£Â Ì‚_ğ‘¡ )

This means that, no matter what scale your inputs are, Adam will take the same step size â€“ only ğ›¼ affects the step size

Now we can finally update our parameter values!

ğœƒ_ğ‘¡=ğœƒ_(ğ‘¡ âˆ’1)âˆ’ã€–ğ›¼_ğ‘¡  â‹… ğ‘šÂ Ì‚_ğ‘¡ã€—âˆ•ã€–(âˆš(ğ‘£Â Ì‚_ğ‘¡ )+ğœ€)ã€—

(ğœ€ is there to prevent dividing by 0)

