# Adam (Adaptive Moment Estimator) Optimizer

## What is Adam?

Adam (Adaptive Moment Estimation) is an algorithm for first-order gradient based optimization of stochastic objective functions based on adaptive estimates of lower-order moments.

Some of Adamâ€™s advantages are:

1. The magnitudes of parameter updates are invariant to rescaling of the gradient 
2. Its stepsizes are approximately bounded by the stepsize hyperparameter 
3. It does not require a stationary objective (i.e., it will still converge if f(Ï‘) changes) 
4. It works with sparse gradients
5. It naturally performs a form of step size annealing

<img width="857" height="536" alt="Pseudocode" src="https://github.com/user-attachments/assets/2747908d-794b-4833-ae26-cd9a42447748" />

<br>

## Moments

The first moment ($$ğ‘š_ğ‘¡$$) is the **mean** of the gradient.

$$ğ‘š_ğ‘¡= {\beta _1} â‹… m_{ğ‘¡âˆ’1}+(1 âˆ’ {\beta _1})â‹… ğ‘”_ğ‘¡$$

The second moment ($$ğ‘£_ğ‘¡$$) is the **raw, uncentered variance** of the gradient.

$$ğ‘£_ğ‘¡= ğ›½_2 â‹… ğ‘£_{ğ‘¡âˆ’1} + (1 âˆ’ {\beta_2})â‹… ğ‘”_ğ‘¡^2$$

The Î²s (decay rates) give more weight to recent gradients.

<br>

However, these moments are **BIASED** early on because they are initialized at 0.

Hence, we have to correct the bias:

$$\hat{ğ‘š_ğ‘¡}= ğ‘š_ğ‘¡âˆ•(1 âˆ’ {\beta_1^ğ‘¡})$$ 
$$\hat{ğ‘£_ğ‘¡}= ğ‘£_ğ‘¡âˆ•(1 âˆ’ {\beta_2^ğ‘¡})$$ 

<br>

But where does this bias correction come from?

First, we rewrite the moment as a function of all previous gradients:

$$ğ‘£_ğ‘¡ = (1 âˆ’ {\beta _2}) \sum_{ğ‘–=1}^ğ‘¡ {\beta_2^{ğ‘¡ âˆ’1}} â‹… ğ‘”_ğ‘–^2$$

We are trying to find the *expected* value of the moment.

$$ğ”¼[ğ‘£_ğ‘¡] = ğ”¼[(1 âˆ’ {\beta _2}) \sum_{ğ‘–=1}^ğ‘¡ {\beta_2^{ğ‘¡ âˆ’1}} â‹… ğ‘”_ğ‘–^2]$$

Next, we assume that the gradient is stationary.

$$ğ”¼[ğ‘£_ğ‘¡] = ğ”¼[ğ‘”_ğ‘–^2] â‹… (1 âˆ’ {\beta _2}) \sum_{ğ‘–=1}^ğ‘¡ {\beta_2^{ğ‘¡ âˆ’1}} + \zeta$$

What the paper fails to make explicit is that the summation here is actually a **geometric series** (i.e., when summing those values, sequential terms are related to each other by a common ratio, in this case, ${\beta_2}$). The summation can also be written as:

$$\sum_{ğ‘–=1}^ğ‘¡ {\beta_2^{ğ‘¡ âˆ’1}} = \frac{1 âˆ’ {\beta_2^ğ‘¡}}{1 âˆ’ {\beta_2}}$$

Therefore, we can sub in $\frac{1 âˆ’ {\beta_2^ğ‘¡}}{1 âˆ’ {\beta_2}}$ for the summation.

$$ğ”¼[ğ‘£_ğ‘¡] = ğ”¼[ğ‘”_ğ‘–^2] â‹… (1 âˆ’ {\beta _2}) \frac{1 âˆ’ {\beta_2^ğ‘¡}}{1 âˆ’ {\beta_2}} + \zeta$$

And then we can arrive at the final equation shown in the paper by cancelling out the $1 - {\beta_2}$:

$$ğ”¼[ğ‘£_ğ‘¡] = ğ”¼[ğ‘”_ğ‘–^2] â‹… (1 âˆ’ {\beta_2^ğ‘¡}) + \zeta$$

So, because of this extra $1 - {\beta_2^t}$ that appears in this equation, that's why we divide $m_t$ and $v_t$ by $1 - {\beta_2^t}$.

<br>

## Step Size

The change in our step size at time t is:

$${\Delta_ğ‘¡} = \alpha â‹… \frac{\hat{ğ‘š_ğ‘¡}}{\sqrt{\hat{ğ‘£_ğ‘¡}}}$$

, where ğ›¼ is a â€œmaximumâ€ step size parameter. If you want to take N steps to the optimum that is D distance away, then $\alpha \approx \frac{ğ·}{ğ‘}$

The effective step sizes are approximately bound to the step size hyperparameter. The step size will not grow too large except in the case of severe sparsity (when a gradient has been zero at all timesteps except at the current timestep), such as in this case:

$$|\Delta_t| \leq \alpha  â‹… \frac{1 - \beta_1}{\sqrt{1 - \beta_2}}, \qquad \text{if }  (1 - \beta_1) \gt \sqrt{1 - \beta_2}$$   

Otherwise, when the gradient isn't sparse:  
  
<p align="center">$$|\Delta_t| \leq \alpha, \qquad \text{if }  (1 - \beta_1) \leq \sqrt{1 - \beta_2}$$   Â  Â  Â  Â 



$\frac{\hat{ğ‘š_ğ‘¡}}{\sqrt{\hat{ğ‘£_ğ‘¡}}}$ is considered to be a signal-to-noise ratio (SNR). When SNR is small, the step size decreases. SNR typically decreases when approaching an optimum, where we want smaller effective steps.

Since the final step equation divides the estimated mean by the estimated variance (1st moment / 2nd moment), any gradient scaling cancels out.â€‹

$$\frac{ğ‘ â‹… \hat{ğ‘š_ğ‘¡}}{\sqrt{(ğ‘^2  â‹… \hat{ğ‘£_ğ‘¡}}}= \frac{\hat{ğ‘š_ğ‘¡}}{\sqrt{\hat{ğ‘£_ğ‘¡}}}$$

This means that, no matter what scale your inputs are, Adam will take the same step size â€“ only $\alpha$ affects the step size.

Now we can finally update our parameter values!

$${\theta_t} = {\theta_{ğ‘¡ âˆ’ 1}} âˆ’ {\alpha _t}  â‹… \frac{\hat{ğ‘š_ğ‘¡}}{\sqrt{\hat{ğ‘£_ğ‘¡}} + \epsilon}$$

(ğœ€ is there to prevent dividing by 0.)

