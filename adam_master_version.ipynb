{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fa37dd",
   "metadata": {},
   "source": [
    "# The Adam optimizer\n",
    "\n",
    "The primary usefulness of the adam optimizer is its improved capabilites over other optimizers in almost all cases. To demonstrate this improvement we are going to implement several different optimizers on a basic toy problem.\n",
    "\n",
    "### But first, some set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "# function for measuring time taken curtises of ChatGPT (!MAY NOT USE, WE SHALL SEE)\n",
    "@contextmanager\n",
    "def time_block(label=\"Elapsed time\"):\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{label}: {end - start:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b590177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My slow code: 0.445547 seconds\n"
     ]
    }
   ],
   "source": [
    "with time_block(\"My slow code\"):\n",
    "    total = sum(i * i for i in range(10_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49075d",
   "metadata": {},
   "source": [
    "### Toy digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944815f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1)) #flattens the data\n",
    "X_train, X_test, y_train, y_test = train_test_split( #splits into training and testing sets\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d9665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec2935",
   "metadata": {},
   "source": [
    "### Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc0cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.classification import Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class classifier_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Linear(input_size,output_size)) # single layer model\n",
    "        \n",
    "    def forward(self,X):\n",
    "        z = self.layer1(X)         # Apply the first layer (and only)\n",
    "        return(z)            # Return the result (and the latent space variable)\n",
    "            \n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "\n",
    "L = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b2b5207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009929288514956924"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training\n",
    "def train_model(L,optimizer,classifier,display=False):\n",
    "    n_epochs = 15\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        for X,y in zip(X_train,y_train): #y are labels\n",
    "            labels = torch.tensor(y)\n",
    "            inputs = torch.from_numpy(X).to(torch.float32)\n",
    "        \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = classifier(inputs) #gets the outputs of the model\n",
    "            train_loss = L(outputs,labels) #gets loss\n",
    "            #print(train_loss)\n",
    "            train_loss.backward() # compute accumulated gradients\n",
    "            optimizer.step()# perform parameter update based on current gradients\n",
    "            loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "        loss = loss / len(X_train) # compute the epoch training loss\n",
    "        if display:\n",
    "            print(f\"epoch : {epoch + 1}/{n_epochs}, loss = {loss}\")\n",
    "    return loss #this is the final loss at the end of all the training process\n",
    "train_model(L,optimizer,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "047d81f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 0.009767960801593905\n",
      "Adam optimizer: 1.351703 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d77318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 0.018097349111964353\n",
      "RMSprop optimizer: 1.147118 seconds\n"
     ]
    }
   ],
   "source": [
    "#torch.optim.LBFGS\n",
    "#torch.optim.RMSprop\n",
    "import torchmin #this is how you get the newtons method full \n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.RMSprop(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"RMSprop optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feaef0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 0.024601183763795054\n",
      "SGD optimizer: 0.868933 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "#train_model(L,optimizer,classifier)\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cedefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 1.4305289820944247\n",
      "Adagrad optimizer: 1.105089 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adagrad(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "#train_model(L,optimizer,classifier)\n",
    "with time_block(\"Adagrad optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128d62c",
   "metadata": {},
   "source": [
    "# The Speed of the hessian matrix\n",
    "\n",
    "One would think that if newtons method is so exact that we would want to use it more often. Unfortunualy the hessian is a slow matrix to calculate as you will see here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccbe84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative df/dx: 12.0\n",
      "First derivative df/dy: 4.0\n",
      "Second derivative d^2f/dx^2: 6.0\n",
      "Mixed derivative d^2f/dydx: 4.0\n",
      "Mixed derivative d^2f/dxdy: 4.0\n"
     ]
    }
   ],
   "source": [
    "#inputs\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "def f(a, b):\n",
    "    return a**2 * b\n",
    "\n",
    "output = f(x, y)\n",
    "df_dx = torch.autograd.grad(outputs=output, inputs=x, create_graph=True)[0]\n",
    "print(f\"First derivative df/dx: {df_dx.item()}\")\n",
    "\n",
    "df_dy = torch.autograd.grad(outputs=output, inputs=y, create_graph=True)[0]\n",
    "print(f\"First derivative df/dy: {df_dy.item()}\")\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Second derivative d^2f/dy^2: {d2f_dy2.item()}\")\n",
    "\n",
    "d2f_dx2 = torch.autograd.grad(outputs=df_dx, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Second derivative d^2f/dx^2: {d2f_dx2.item()}\")\n",
    "\n",
    "d2f_dydx = torch.autograd.grad(outputs=df_dx, inputs=y,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dydx: {d2f_dydx.item()}\")\n",
    "\n",
    "d2f_dxdy = torch.autograd.grad(outputs=df_dy, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dxdy: {d2f_dxdy.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Mixed derivative d2f_dy2: {d2f_dy2.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59aaf8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci457",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
