{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fa37dd",
   "metadata": {},
   "source": [
    "# The Adam optimizer\n",
    "\n",
    "The primary usefulness of the adam optimizer is its improved capabilites over other optimizers in almost all cases. To demonstrate this improvement we are going to implement several different optimizers on a basic toy problem.\n",
    "\n",
    "### But first, some set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "# function for measuring time taken curtises of ChatGPT (!MAY NOT USE, WE SHALL SEE)\n",
    "@contextmanager\n",
    "def time_block(label=\"Elapsed time\"):\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{label}: {end - start:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b590177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My slow code: 0.445547 seconds\n"
     ]
    }
   ],
   "source": [
    "with time_block(\"My slow code\"):\n",
    "    total = sum(i * i for i in range(10_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49075d",
   "metadata": {},
   "source": [
    "### Toy digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944815f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1)) #flattens the data\n",
    "X_train, X_test, y_train, y_test = train_test_split( #splits into training and testing sets\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d9665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec2935",
   "metadata": {},
   "source": [
    "### Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc0cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.classification import Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class classifier_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Linear(input_size,output_size)) # single layer model\n",
    "        \n",
    "    def forward(self,X):\n",
    "        z = self.layer1(X)         # Apply the first layer (and only)\n",
    "        return(z)            # Return the result (and the latent space variable)\n",
    "            \n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "\n",
    "L = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b2b5207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009929288514956924"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training\n",
    "def train_model(L,optimizer,classifier,display=False):\n",
    "    n_epochs = 15\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        for X,y in zip(X_train,y_train): #y are labels\n",
    "            labels = torch.tensor(y)\n",
    "            inputs = torch.from_numpy(X).to(torch.float32)\n",
    "        \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = classifier(inputs) #gets the outputs of the model\n",
    "            train_loss = L(outputs,labels) #gets loss\n",
    "            #print(train_loss)\n",
    "            train_loss.backward() # compute accumulated gradients\n",
    "            optimizer.step()# perform parameter update based on current gradients\n",
    "            loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "        loss = loss / len(X_train) # compute the epoch training loss\n",
    "        if display:\n",
    "            print(f\"epoch : {epoch + 1}/{n_epochs}, loss = {loss}\")\n",
    "    return loss #this is the final loss at the end of all the training process\n",
    "train_model(L,optimizer,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "047d81f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 0.009767960801593905\n",
      "Adam optimizer: 1.351703 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d77318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 0.018097349111964353\n",
      "RMSprop optimizer: 1.147118 seconds\n"
     ]
    }
   ],
   "source": [
    "#torch.optim.LBFGS\n",
    "#torch.optim.RMSprop\n",
    "import torchmin #this is how you get the newtons method full \n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.RMSprop(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"RMSprop optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feaef0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 0.024601183763795054\n",
      "SGD optimizer: 0.868933 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "#train_model(L,optimizer,classifier)\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cedefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the final loss of the model: 1.4305289820944247\n",
      "Adagrad optimizer: 1.105089 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adagrad(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "#train_model(L,optimizer,classifier)\n",
    "with time_block(\"Adagrad optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128d62c",
   "metadata": {},
   "source": [
    "# The Speed of the hessian matrix\n",
    "\n",
    "One would think that if newtons method is so exact that we would want to use it more often. Unfortunualy the hessian is a slow matrix to calculate as you will see here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccbe84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative df/dx: 12.0\n",
      "First derivative df/dy: 4.0\n",
      "Second derivative d^2f/dx^2: 6.0\n",
      "Mixed derivative d^2f/dydx: 4.0\n",
      "Mixed derivative d^2f/dxdy: 4.0\n"
     ]
    }
   ],
   "source": [
    "#inputs\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "def f(a, b):\n",
    "    return a**2 * b\n",
    "\n",
    "output = f(x, y)\n",
    "df_dx = torch.autograd.grad(outputs=output, inputs=x, create_graph=True)[0]\n",
    "print(f\"First derivative df/dx: {df_dx.item()}\")\n",
    "\n",
    "df_dy = torch.autograd.grad(outputs=output, inputs=y, create_graph=True)[0]\n",
    "print(f\"First derivative df/dy: {df_dy.item()}\")\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Second derivative d^2f/dy^2: {d2f_dy2.item()}\")\n",
    "\n",
    "d2f_dx2 = torch.autograd.grad(outputs=df_dx, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Second derivative d^2f/dx^2: {d2f_dx2.item()}\")\n",
    "\n",
    "d2f_dydx = torch.autograd.grad(outputs=df_dx, inputs=y,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dydx: {d2f_dydx.item()}\")\n",
    "\n",
    "d2f_dxdy = torch.autograd.grad(outputs=df_dy, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dxdy: {d2f_dxdy.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Mixed derivative d2f_dy2: {d2f_dy2.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59aaf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b3655b",
   "metadata": {},
   "source": [
    "Some sore of activtiyy where they calucate the full hessian, the code isnt finalized or even working right now but thats the concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40d62d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_functional' from 'torch.func' (/Users/noahwanless/Desktop/Spring2026/CSCI457/.venv/lib/python3.11/site-packages/torch/func/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m targets = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 3. Extract parameters as a flat dictionary or tuple (functorch format)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Note: make_functional is a utility to easily get functional model and params\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_functional\n\u001b[32m     20\u001b[39m fnet, params = make_functional(model)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 4. Compute the Hessian\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# We need to wrap the loss computation in a function that takes params as input\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'make_functional' from 'torch.func' (/Users/noahwanless/Desktop/Spring2026/CSCI457/.venv/lib/python3.11/site-packages/torch/func/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import hessian, jacfwd, jacrev\n",
    "\n",
    "# 1. Define a function that returns a single scalar loss\n",
    "def compute_loss(model_params, input_data, targets, model, loss_fn):\n",
    "    # Use functional_call to call the model with specific parameters\n",
    "    outputs = torch.func.functional_call(model, model_params, input_data)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    return loss\n",
    "\n",
    "# 2. Instantiate your model and loss function\n",
    "model = torch.nn.Linear(10, 1) # Example model\n",
    "loss_fn = torch.nn.MSELoss()    # Example loss function\n",
    "input_data = torch.randn(1, 10)\n",
    "targets = torch.randn(1, 1)\n",
    "\n",
    "# 3. Extract parameters as a flat dictionary or tuple (functorch format)\n",
    "# Note: make_functional is a utility to easily get functional model and params\n",
    "from torch.func import make_functional\n",
    "fnet, params = make_functional(model)\n",
    "\n",
    "# 4. Compute the Hessian\n",
    "# We need to wrap the loss computation in a function that takes params as input\n",
    "def get_hessian_func(params):\n",
    "    return compute_loss(params, input_data, targets, model, loss_fn)\n",
    "\n",
    "# Calculate the full Hessian matrix\n",
    "# argnums=0 specifies we want the Hessian with respect to the first argument (params)\n",
    "full_hessian = hessian(get_hessian_func, argnums=0)(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36eb9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70eae01d",
   "metadata": {},
   "source": [
    "The Hessian encodes local curvature of the objective function, it allows the gradient to be scaled so that 'dierctions' that have little movement are downscaled, while directions with more curvuture are scaled up so more progress is made"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci457",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
