{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fa37dd",
   "metadata": {},
   "source": [
    "# The Adam optimizer\n",
    "\n",
    "The primary usefulness of the adam optimizer is its improved capabilites over other optimizers in almost all cases. To demonstrate this improvement we are going to implement several different optimizers on a basic toy problem.\n",
    "But first, some set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9d8b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.classification import Accuracy\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.classification import Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d8bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for measuring time taken curtises of ChatGPT \n",
    "@contextmanager\n",
    "def time_block(label=\"Elapsed time\"):\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{label} elapsed time: {end - start:.6f} seconds\")\n",
    "\n",
    "# function for measuring accuracy\n",
    "def get_acc(X_test, y_test,classifier):\n",
    "    num_classes = 10\n",
    "    acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = classifier(torch.from_numpy(X_test).float())  # [899, 10] \n",
    "        y_true = torch.tensor(y_test)                           # [899]\n",
    "        # Convert logits â†’ predicted class indices\n",
    "        y_pred_labels = torch.argmax(y_pred, dim=1)             # [899]\n",
    "    accuracy = acc(y_pred_labels, y_true).item()\n",
    "    print(\n",
    "        f\"Final accuracy: {accuracy:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49075d",
   "metadata": {},
   "source": [
    "### Toy digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "944815f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1)) #flattens the data\n",
    "X_train, X_test, y_train, y_test = train_test_split( #splits into training and testing sets\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec2935",
   "metadata": {},
   "source": [
    "### Basic Classification Model\n",
    "\n",
    "Now we are going to define a classification model that we are going to use, pretty basic, but we will be changing up the optimizer on this model to compare perforamces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bc0cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Linear(input_size,output_size)) # single layer model\n",
    "        \n",
    "    def forward(self,X):\n",
    "        z = self.layer1(X)         # Apply the first layer (and only)\n",
    "        return(z)            # Return the result (and the latent space variable\n",
    "\n",
    "#training a model automatically\n",
    "def train_model(L,optimizer,classifier, X_train, y_train, display=False):\n",
    "    n_epochs = 15\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        for X,y in zip(X_train,y_train): #y are labels\n",
    "            labels = torch.tensor(y)\n",
    "            inputs = torch.from_numpy(X).to(torch.float32)\n",
    "        \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = classifier(inputs) #gets the outputs of the model\n",
    "            train_loss = L(outputs,labels) #gets loss\n",
    "            train_loss.backward() # compute accumulated gradients\n",
    "            optimizer.step()# perform parameter update based on current gradients\n",
    "            loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "        loss = loss / len(X_train) # compute the epoch training loss\n",
    "        if display:\n",
    "            print(f\"epoch : {epoch + 1}/{n_epochs}, loss = {loss}\")\n",
    "    return loss #this is the final loss at the end of all the training process\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b2b5207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008695103159739067"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "train_model(L,optimizer,classifier, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d18794-0d0d-4c5b-95f4-a315dbf68395",
   "metadata": {},
   "source": [
    "### Gradient Scaling\n",
    "Unlike some basic optimizers, the Adam Optimizer is invariant to gradient scaling, so the default learning rate works well for most datasets. Run the optimizer on the dataset scaled datasets and compare the final accuracy. Is there any scalar that causes a noticable decrease in performance? Then, try the same (or different) scalars on Stochastic Gradient Descent. When does accuracy start to drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "047d81f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9066\n",
      "This is the final loss of the model: 0.0106 \n",
      "Adam optimizer elapsed time: 1.242114 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "\"\"\"\n",
    "Run the classifier with the Adam Optimizer here\n",
    "?remove the next line? \n",
    "\"\"\" \n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train, y_train)\n",
    "    get_acc(X_test, y_test,classifier)\n",
    "    print(\n",
    "        f\"This is the final loss of the model: {final_loss:.4f} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c58cc4be-3071-4fea-ab15-8dcf6014d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9043\n",
      "Adam optimizer elapsed time: 1.282248 seconds\n",
      "Final accuracy: 0.9055\n",
      "Adam optimizer elapsed time: 1.233077 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scale the training/testing data. Compare the accuracies.\n",
    "\"\"\"\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "X_train_scaled = X_train/50\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train_scaled, y_train)\n",
    "    get_acc(X_test/50, y_test,classifier)\n",
    "\n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "X_train_scaled = X_train*50\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train_scaled, y_train)\n",
    "    get_acc(X_test*50, y_test,classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ded60-c060-4426-8910-e3c5f8c9480c",
   "metadata": {},
   "source": [
    "### Compare this with regular Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71d3726a-7c13-4375-82b4-fa5e142195ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9099\n",
      "SGD optimizer elapsed time: 0.874985 seconds\n",
      "Final accuracy: 0.0945\n",
      "SGD optimizer elapsed time: 0.809412 seconds\n",
      "Final accuracy: 0.1346\n",
      "SGD optimizer elapsed time: 0.811626 seconds\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "\"\"\"\n",
    "Now, run Stochastic Gradient Descent with at least twice with different scalars. Compare the change in accuracy to the Adam Optimizer.\n",
    "\"\"\"\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=1e-3) \n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train*(1/5), y_train)\n",
    "    get_acc(X_test*(1/5), y_test,classifier)\n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train*(5), y_train)\n",
    "    get_acc(X_test*(5), y_test,classifier)\n",
    "\n",
    "classifier = classifier_model(64,10) #defines the model (!WITHOUT SCALING)\n",
    "with time_block(\"SGD optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train, y_train)\n",
    "    get_acc(X_test, y_test,classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbf55b-50c5-4f18-b0b1-b244e5e275df",
   "metadata": {},
   "source": [
    "### If scaling does not affect the data, what about normalization? Compare the accuracy of the model before and after normalization. As a bonus, implement both with cross validation for more stable results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "864b700d-6b8c-47a0-ab9b-9bd61af37961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9232\n",
      "Adam optimizer elapsed time: 1.242129 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Normalize the training/testing data and run the classifier using the Adam optimizer\n",
    "\"\"\"\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mean = np.mean(X_train, axis=0, keepdims=True) \n",
    "std  = np.std(X_train, axis=0, keepdims=True)   \n",
    "\n",
    "eps = 1e-8\n",
    "X_train_scaled = (X_train - mean) / (std + eps)\n",
    "X_test_scaled  = (X_test  - mean) / (std + eps)\n",
    "\n",
    "\n",
    "with time_block(\"Adam optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train_scaled, y_train)\n",
    "    get_acc(X_test_scaled, y_test,classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8506ae-925a-439e-9ce9-e94b1821fa7a",
   "metadata": {},
   "source": [
    "### The Adam Optimizer is based off of both RMSProp and AdaGrad.\n",
    "Empirically, is there any difference in accuracy for this basic dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d77318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9188\n",
      "RMSprop optimizer elapsed time: 1.111017 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the classifier with RMSprop using the built-in torch function\n",
    "\"\"\"\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.RMSprop(classifier.parameters(), lr=1e-3)\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "with time_block(\"RMSprop optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train, y_train)\n",
    "    get_acc(X_test, y_test,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cedefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9177\n",
      "This is the final loss of the model: 0.09698407760616795\n",
      "Adagrad optimizer elapsed time: 1.077773 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the classifier with Adagrad using the built-in torch function. Adaprop specifically is known to have issues with step size decreasing too drastically. \n",
    "Does accuracy improve with a different learning rate?\n",
    "\"\"\"\n",
    "classifier = classifier_model(64,10) #defines the model\n",
    "##################### Have people do?\n",
    "optimizer = torch.optim.Adagrad(classifier.parameters(), lr=1e-2)\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#####################\n",
    "#train_model(L,optimizer,classifier)\n",
    "with time_block(\"Adagrad optimizer\"):\n",
    "    final_loss = train_model(L,optimizer,classifier, X_train, y_train)\n",
    "    get_acc(X_test, y_test,classifier)\n",
    "    print(f\"This is the final loss of the model: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1506a",
   "metadata": {},
   "source": [
    "# Newtons method\n",
    "\n",
    "What Adam does is related to a method called Newtons method.\n",
    "\n",
    "Newtons method works by calcuating local curvuture information using second partial derivatives of your objective function to get a sense of what directions you can speed along, and what ones you want to move more carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05e67d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchmin.optim import MinimizeWrapper\n",
    "import torchmin\n",
    "#import pytorch_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c67b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_model(64,10) #defines the model\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) #adam optimizer for the gradient decent\n",
    "L = torch.nn.CrossEntropyLoss()\n",
    "#torchmin.newton._minimize_newton_exact()\n",
    "#from pytorch_minimize.optim import MinimizeWrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb400f4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'numel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorchmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewton\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_minimize_newton_exact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Use this somehow?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring2026/CSCI457/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring2026/CSCI457/.venv/lib/python3.11/site-packages/torchmin/newton.py:283\u001b[39m, in \u001b[36m_minimize_newton_exact\u001b[39m\u001b[34m(fun, x0, lr, max_iter, line_search, xtol, normp, tikhonov, handle_npd, callback, disp, return_all)\u001b[39m\n\u001b[32m    281\u001b[39m lr = \u001b[38;5;28mfloat\u001b[39m(lr)\n\u001b[32m    282\u001b[39m disp = \u001b[38;5;28mint\u001b[39m(disp)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m xtol = \u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumel\u001b[49m() * xtol\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    285\u001b[39m     max_iter = x0.numel() * \u001b[32m200\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'generator' object has no attribute 'numel'"
     ]
    }
   ],
   "source": [
    "torchmin.newton._minimize_newton_exact(L(torch.from_numpy(X_train),torch.from_numpy(y_train)),) #Use this somehow? #!ASK DOUG MABYEder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "607c4b9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinimizeWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimizer = \u001b[43mMinimizeWrapper\u001b[49m(\n\u001b[32m      2\u001b[39m     model.parameters(),\n\u001b[32m      3\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mnewton-cg\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     tol=\u001b[32m1e-6\u001b[39m,\n\u001b[32m      5\u001b[39m     options={\u001b[33m\"\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m20\u001b[39m}\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m():\n\u001b[32m      9\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[31mNameError\u001b[39m: name 'MinimizeWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = MinimizeWrapper(\n",
    "    model.parameters(),\n",
    "    method=\"newton-cg\",\n",
    "    tol=1e-6,\n",
    "    options={\"maxiter\": 20}\n",
    ")\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(X)\n",
    "    loss = criterion(preds, y)\n",
    "    loss.backward(create_graph=True)\n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7debea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "for epoch in range(n_epochs):\n",
    "    loss = 0\n",
    "    for X,y in zip(X_train,y_train): #y are labels\n",
    "        labels = torch.tensor(y)\n",
    "        inputs = torch.from_numpy(X).to(torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = classifier(inputs) #gets the outputs of the model\n",
    "        train_loss = L(outputs,labels) #gets loss\n",
    "        train_loss.backward() # compute accumulated gradients\n",
    "        optimizer.step()# perform parameter update based on current gradients\n",
    "        loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "    loss = loss / len(X_train) # compute the epoch training loss\n",
    "    if display:\n",
    "        print(f\"epoch : {epoch + 1}/{n_epochs}, loss = {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390d857",
   "metadata": {},
   "source": [
    "One would think that if newtons method is so exact that we would want to use it more often. Unfortunualy the hessian is a slow matrix to calculate. As you more then likely noticed, using the full hessian is time consuming so we dont want to do that, additionally we have a dimensionality problem as you will see here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128d62c",
   "metadata": {},
   "source": [
    "# The Speed of the hessian matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbe84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative df/dx: 12.0\n",
      "First derivative df/dy: 4.0\n",
      "Second derivative d^2f/dx^2: 6.0\n",
      "Mixed derivative d^2f/dydx: 4.0\n",
      "Mixed derivative d^2f/dxdy: 4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###############################################delete this cell of code?\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "def f(a, b):\n",
    "    return a**2 * b\n",
    "\n",
    "output = f(x, y)\n",
    "df_dx = torch.autograd.grad(outputs=output, inputs=x, create_graph=True)[0]\n",
    "print(f\"First derivative df/dx: {df_dx.item()}\")\n",
    "\n",
    "df_dy = torch.autograd.grad(outputs=output, inputs=y, create_graph=True)[0]\n",
    "print(f\"First derivative df/dy: {df_dy.item()}\")\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Second derivative d^2f/dy^2: {d2f_dy2.item()}\")\n",
    "\n",
    "d2f_dx2 = torch.autograd.grad(outputs=df_dx, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Second derivative d^2f/dx^2: {d2f_dx2.item()}\")\n",
    "\n",
    "d2f_dydx = torch.autograd.grad(outputs=df_dx, inputs=y,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dydx: {d2f_dydx.item()}\")\n",
    "\n",
    "d2f_dxdy = torch.autograd.grad(outputs=df_dy, inputs=x,retain_graph=True)[0]\n",
    "print(f\"Mixed derivative d^2f/dxdy: {d2f_dxdy.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "#d2f_dy2 = torch.autograd.grad(outputs=df_dy, inputs=y,retain_graph=True,allow_unused=True)[0]\n",
    "#print(f\"Mixed derivative d2f_dy2: {d2f_dy2.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59aaf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b3655b",
   "metadata": {},
   "source": [
    "Some sore of activtiyy where they calucate the full hessian, the code isnt finalized or even working right now but thats the concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40d62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1. Define a function that returns a single scalar loss\\ndef compute_loss(model_params, input_data, targets, model, loss_fn):\\n    # Use functional_call to call the model with specific parameters\\n    outputs = torch.func.functional_call(model, model_params, input_data)\\n    loss = loss_fn(outputs, targets)\\n    return loss\\n\\n# 2. Instantiate your model and loss function\\nmodel = torch.nn.Linear(10, 1) # Example model\\nloss_fn = torch.nn.MSELoss()    # Example loss function\\ninput_data = torch.randn(1, 10)\\ntargets = torch.randn(1, 1)\\n\\n# 3. Extract parameters as a flat dictionary or tuple (functorch format)\\n# Note: make_functional is a utility to easily get functional model and params\\nfrom torch.func import make_functional\\nfnet, params = make_functional(model)\\n\\n# 4. Compute the Hessian\\n# We need to wrap the loss computation in a function that takes params as input\\ndef get_hessian_func(params):\\n    return compute_loss(params, input_data, targets, model, loss_fn)\\n\\n# Calculate the full Hessian matrix\\n# argnums=0 specifies we want the Hessian with respect to the first argument (params)\\nfull_hessian = hessian(get_hessian_func, argnums=0)(params)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import hessian, jacfwd, jacrev\n",
    "'''\n",
    "# 1. Define a function that returns a single scalar loss\n",
    "def compute_loss(model_params, input_data, targets, model, loss_fn):\n",
    "    # Use functional_call to call the model with specific parameters\n",
    "    outputs = torch.func.functional_call(model, model_params, input_data)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    return loss\n",
    "\n",
    "# 2. Instantiate your model and loss function\n",
    "model = torch.nn.Linear(10, 1) # Example model\n",
    "loss_fn = torch.nn.MSELoss()    # Example loss function\n",
    "input_data = torch.randn(1, 10)\n",
    "targets = torch.randn(1, 1)\n",
    "\n",
    "# 3. Extract parameters as a flat dictionary or tuple (functorch format)\n",
    "# Note: make_functional is a utility to easily get functional model and params\n",
    "from torch.func import make_functional\n",
    "fnet, params = make_functional(model)\n",
    "\n",
    "# 4. Compute the Hessian\n",
    "# We need to wrap the loss computation in a function that takes params as input\n",
    "def get_hessian_func(params):\n",
    "    return compute_loss(params, input_data, targets, model, loss_fn)\n",
    "\n",
    "# Calculate the full Hessian matrix\n",
    "# argnums=0 specifies we want the Hessian with respect to the first argument (params)\n",
    "full_hessian = hessian(get_hessian_func, argnums=0)(params)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36eb9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8283, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.1660, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.9312,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.3379,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.1332]])\n"
     ]
    }
   ],
   "source": [
    "def test_func(x):\n",
    "    return (x.sin()).sum()\n",
    "x = torch.randn(5) #takes in 5 inputs, so 5x5 hessian\n",
    "y = torch.randn(5)\n",
    "\n",
    "full_hessian = hessian(test_func, argnums=0)(x)\n",
    "print(full_hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eae01d",
   "metadata": {},
   "source": [
    "The Hessian encodes local curvature of the objective function, it allows the gradient to be scaled so that 'dierctions' that have little movement are downscaled, while directions with more curvuture are scaled up so more progress is made.\n",
    "\n",
    "However this means in high dimensions we are calcuating THOUSANDS of values to figure out this curvture, and for full felgded models, this is simply not computationally doable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f03ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: Optimization terminated successfully.\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: tensor(-5.)\n",
       "       x: tensor([-1.5708, -1.5708, -1.5708, -1.5708, -1.5708])\n",
       "     nit: 6\n",
       "    grad: tensor([-4.3711e-08, -4.3711e-08, -4.3711e-08, -4.3711e-08, -4.3711e-08])\n",
       "    hess: tensor([[1., 0., 0., 0., 0.],\n",
       "                  [0., 1., 0., 0., 0.],\n",
       "                  [0., 0., 1., 0., 0.],\n",
       "                  [0., 0., 0., 1., 0.],\n",
       "                  [0., 0., 0., 0., 1.]])\n",
       "    nfev: 15\n",
       "   nfail: 2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchmin.newton._minimize_newton_exact(test_func,torch.randn(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04e5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci457",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
